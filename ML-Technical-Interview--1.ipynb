{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc07f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peter Wu\n",
    "# qw262@corenll.edu\n",
    "# 1/22/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5c0a3",
   "metadata": {},
   "source": [
    "### Suppose that we design a deep architecture to represent a sequence by stacking self-attention layers with positional encoding. What could be issues? (paragraph format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a912cb",
   "metadata": {},
   "source": [
    "#### Sequence model (transformers) and self-attention layers explained, using  a natural language (text) example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b87f17",
   "metadata": {},
   "source": [
    "1. Sequence model: e.g, a transformer\n",
    "1. Positional encoding: e.g, position of words in a sentence\n",
    "1. **Self-attention layers (KQV attention)**: key: input; query: output; value: a learned/calculated vector -> together they form a attention head\n",
    "    1. Encoder: what we have (e.g, a sentence)\n",
    "        1. Word-2-vec + positional encoding: a R<sup>128</sup> vector to represent a word x<sub>i</sub>\n",
    "        1. Calculate attention of each pair: with key and query, use Softmax( (Q * K) / Scale) to get a distribution of each pair of wrods; now knows the releation between each pair\n",
    "        1. Multiply attention with value: we get x<sup>'</sup><sub>i</sub>\n",
    "        1. Repeat the steps: we can get a multi-head self-attention (**stacking self-attention layers**), by for example, using a fully connected neural network: x = FC(x<sup>''</sup><sub>i</sub> , x<sup>'</sup><sub>i</sub> , x<sub>i</sub> , ...) \n",
    "    1. Decoder: what we want to predict/match (e.g, the next word)\n",
    "        1. Cross-attention: look through the past, relation between a current word and words already occurred <- same steps as self-attention\n",
    "        2. **Find the next word: from the English vocabulary, which are the most likely words, given cross-attention?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96061bd",
   "metadata": {},
   "source": [
    "#### Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f6463",
   "metadata": {},
   "source": [
    "**Interpretability**: just like other NNs, although stacking layers make lingustic sense (there are many relationships between words in a sentence), it is hard to explain why the layers are necessary and how they form a decison-making process.\n",
    "\n",
    "**Overfitting**: depend on the training data, we can have high variance if the model overfits, i.e., stacking more then necesary self-attnetion layers. A limited amount of training data, together with two many layers, can lead to a over-explaination of the data. The model finds a relationship that does not exist, or can not be generalized.\n",
    "\n",
    "**Time/Space Complexity**: large language model often have an enormous amount of parameters, and use corpus to train also takes time. Training, testing, and storing the model demands a lot of resources. \n",
    "\n",
    "**Positional Encoding**: one inherent limitation, for longer sequences(sentences), the positons of words becomes difficult to interpret, and simply stacking layers may not help. May need more transforming procedures before sending it to self-attention layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce16cd",
   "metadata": {},
   "source": [
    "Referenced Sources:\n",
    "- Cornell CS 5740 Intro to Machine Learing (taken last year)\n",
    "- https://arxiv.org/pdf/1706.03762.pdf\n",
    "- https://medium.com/analytics-vidhya/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbffafa",
   "metadata": {},
   "source": [
    "### Can you design a learnable positional encoding method using pytorch? (Create dummy dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0867e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc376ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Create dummy dataset, inherit pytorch dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim=10, seq_len=10, num_seq=3000):\n",
    "        \"\"\"Constructor to instantiate\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim : dimension of the positional encoding\n",
    "        seq_length: length of sequence (# of words)\n",
    "        num_seq: number of sequences\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_seq = num_seq\n",
    "        \n",
    "        # generate data from a random normal distrubiton, with mean 0, var 1\n",
    "        self.data = torch.randn(num_seq, seq_len, dim) \n",
    "        \n",
    "    def __getitem__(self, id):\n",
    "        \"\"\"get a sequece in the dataset by id\n",
    "            inherited from data.Dataset\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        id : dataset[id]\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.data[id], torch.tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433be41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Create pos-encoding, inherit the base nn\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, max_seq=500):\n",
    "        \"\"\"Constructor to instantiate\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim : dimension of the positional encoding, same as DummyDataset.dim\n",
    "        max_seq: max length of a sequence\n",
    "        \"\"\"\n",
    "        \n",
    "        super(PositionalEncoding, self).__init__() \n",
    "        \n",
    "        # create the pos-encoding tensors <- learnable\n",
    "        self.pos_encode = nn.Parameter(torch.zeros(max_seq, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : new sequence to update encoding,\n",
    "        Here the encoding here is simple,\n",
    "        however,\n",
    "        it can be sth like a sin(x)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(dim=1) # obtain seq length\n",
    "        \n",
    "        # get the positions of the sequence\n",
    "        pos = torch.arange(seq_len)\n",
    "        \n",
    "        # get the pos-encoding with dim, from the learnable self.pos_encode\n",
    "        pos_encode = self.pos_encode[pos, :]\n",
    "                \n",
    "        #return word-2-vec + positional encoding\n",
    "        return x + pos_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ddab013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Create a linear model, inherit the base nn\n",
    "    Predict the mean of a sequence's encoding\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, dim=10, seq_len=10):\n",
    "        \"\"\"Constructor to instantiate\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim : dimension of the positional encoding, same as DummyDataset.dim\n",
    "        max_seq: max length of a sequence\n",
    "        \"\"\"\n",
    "            \n",
    "        super(LinearModel, self).__init__()\n",
    "        \n",
    "        # use pos-encoding in the linear model <- learnable\n",
    "        self.pos_encode = PositionalEncoding(dim, seq_len)\n",
    "        \n",
    "        # a 1 layer model <- learnable\n",
    "        self.layer = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : new sequence to update model\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.pos_encode(x) # add learnable pos-encoding\n",
    "        output = self.layer(x.mean(dim=1)) # output for the learnable layer, mean of value\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1a148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # fix seed\n",
    "dataset = DummyDataset()\n",
    "model = LinearModel(dim=10, seq_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497fd0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0671]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0880)\n"
     ]
    }
   ],
   "source": [
    "dummy_input = dataset[0][0].unsqueeze(0)  \n",
    "output = model(dummy_input)\n",
    "print(output)\n",
    "print(dataset[0][0].unsqueeze(0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771d2aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0436]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0395)\n"
     ]
    }
   ],
   "source": [
    "dummy_input = dataset[1][0].unsqueeze(0) \n",
    "output = model(dummy_input)\n",
    "print(output)\n",
    "print(dataset[1][0].unsqueeze(0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be19594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2207]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0030)\n"
     ]
    }
   ],
   "source": [
    "dummy_input = dataset[2][0].unsqueeze(0)\n",
    "output = model(dummy_input)\n",
    "print(output)\n",
    "print(dataset[2][0].unsqueeze(0).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
